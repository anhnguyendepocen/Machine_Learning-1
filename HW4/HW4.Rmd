---
title: "HW4"
author: "Sheng Zhang"
date: "April 23, 2017"
output: pdf_document
---

```{r Q1}

## Q1

# Read in data
advertising <- read.csv("./Advertising.csv", header = TRUE)
# advertising <- read.csv("./Spring 2017/Machine Learning/Rmd files/HW4/Advertising.csv", header = TRUE)
advertising <- scale(advertising[,-1]) # standardize all inputs to have zero mean and unit variance


# Sample training and test sets
set.seed(1)
advertising.selection.id <- sample(1:nrow(advertising), 200)
advertising.train.id <- sample(advertising.selection.id, 150)
advertising.test.id <- advertising.selection.id[-advertising.train.id]
advertising.train <- advertising[advertising.train.id, ]
advertising.test <- advertising[advertising.test.id, ]
x.train <- advertising.train[,-4]
y.train <- advertising.train[,4]
x.test <- advertising.test[,-4]
y.test <- advertising.test[,4]

# a)
# Build a one-hidden-layer neural network
library(RSNNS)
advertising.nn1 <- RSNNS::mlp(x=x.train,y=y.train,size = c(2), maxit = 10000, learnFuncParams = 0.01, linOut = TRUE )
summary(advertising.nn1)
nn1.pred <- predict(advertising.nn1,x.test)
mse <- sum((nn1.pred-y.test)^2)/length(y.test)
mse

# Plot the results
plot(y.test)
points(nn1.pred, col = "red", pch = 3)

```

1)

a) The results are shown from the R output above.

b) I chose 2 as the number of hidden units because there are 3 input units and 1 output unit, so 2, which is between 3 and 1, is an appropriate choice for the number of hidden units. Alternatively, I could choose 3 as the number of hidden units and look at the model results with regularization. For the learning parameter, I selected 0.01 after comparing model performance with difference learning rates, but could use cross validation to select the best learning parameter as well. 


```{r Q2}

## Q2

# Specify 9 kinds of hidden unit configurations
size_list <- list(c(2),c(3),c(3,3),c(3,2),c(2,2),c(3,3,3),c(3,3,2),c(3,2,2),c(2,2,2))
nn.pred <- nn1.pred

for (i in 1:9)
{
  for (j in 1:4) # Use 4 different starting values for each configuration
  {
    advertising.nn_temp <- RSNNS::mlp(x=x.train,y=y.train,size = size_list[[i]], maxit = 10000, learnFuncParams = 0.01, linOut = TRUE )
    nn.pred <- cbind(nn.pred, predict(advertising.nn_temp,x.test))
  }
}

# Select 30 models from 36 models estimated
nn.pred <- nn.pred[,-1]
selection.id <- sample(1:36,30)
nn.pred <- nn.pred[,selection.id]

# Calculate the ensemble predictions and calculate MSE
nn_ensemble.pred <- rowMeans(nn.pred)
mse_ensemble <- sum((nn_ensemble.pred-y.test)^2)/length(y.test)
mse_ensemble

```

2)

I selected 30 neural networks by varying both hidden layer configuration (number of hidden of layers and number of hidden units in each layer) and starting values. The MSE of the ensemble method turns out to be about 0.0062, which is much smaller than the MSE obtained from Q1, suggesting the ensemble method improves the predictve accuracy by a lot.

The reason for the improvement is perhaps because calibrated weights will depend on the chosen random starting points and averaging predictions with different starting points reduce the variance by a lot.


```{r Q3}

## Q3

library(rugarch)
data("dji30ret")
summary(dji30ret)

# Construct the data matrix with Ys and one-lag Xs
dji30ret$AVG <- rowMeans(dji30ret)
summary(dji30ret)
xfactors <- dji30ret[-nrow(dji30ret),-ncol(dji30ret)]
yfactors <- dji30ret[-1,ncol(dji30ret)]
dji_data <- as.matrix(cbind(xfactors,yfactors))
dji_data <- scale(dji_data)

# a)
# Split into training and test sets
split_date.id <- which(dji_data[,1]==dji_data["2005-12-30",1]) + 1
dji_train <- dji_data[1:split_date.id,]
dji_test <- dji_data[split_date.id:nrow(dji_data),]

# Fit a Elman neural network
library(RSNNS)
x_train <- dji_train[,-ncol(dji_train)]
y_train <- dji_train[,ncol(dji_train)]
dji.elman <- elman(x_train, y_train, size = c(5), learnFuncParams = c(0.1), maxit = 1000)
plotIterativeError(dji.elman)
summary(dji.elman)

# b)
# Make predictions and calculate MSE
x_test <- dji_test[,-ncol(dji_test)]
y_test <- dji_test[,ncol(dji_test)]
elman.pred <- predict(dji.elman, x_test)
mse_elman <- sum((elman.pred-y_test)^2)/length(y_test)
mse_elman

```

3)

a) The results are shown from the R output above.

b) The results are shown from the R output above.

c) Elman neural network is an appropriate model for time-series prediction, as the context layer within the Elman neural network "remember" the previous internal state of the network by storing hidden layer neuron values. The number of hidden units chosen for the model is 5, much fewer than the number of input units, which is 30, suggesting that overfitting may not occur. We could also use regularization to minimize the risk of overfitting.


```{r Q4}

## Q4
# Build transformed dataset with 5 lagged returns
library(ISLR)
data("Weekly")
summary(Weekly)

# Split into training and test sets
sp_500.x_train <- head(scale(as.matrix(Weekly[,2:6])),889)
sp_500.y_train <- head(scale(as.matrix(Weekly[,8])),889)

# Build the Deep Belief Net (DBN)
library(deepnet)
sp_500.dbn <- dbn.dnn.train(sp_500.x_train, sp_500.y_train, hidden = c(4,3), output = "linear", learningrate = 0.01, numepochs = 1000, cd = 10, momentum = 0.6, hidden_dropout = 0.4, visible_dropout = 0.2, batchsize = 10)

# Make predictions and calculate MSE
sp_500.x_test <- tail(scale(as.matrix(Weekly[,2:6])),200)
sp_500.y_test <- tail(scale(as.matrix(Weekly[,8])),200)
dbn.pred <- nn.predict(sp_500.dbn, sp_500.x_test)
mse_dbn <- sum((dbn.pred-sp_500.y_test)^2)/length(sp_500.y_test)
mse_dbn

```

4) 

a) The results are shown from the R output above.

b) The results are shown from the R output above.

c) I used c(4,3) as the number of hidden units because the number of inputs is 5, so I want to make the number of hidden units smaller or equal to 5 to extract meaningful features from the input. I also tried many other hidden layer configurations that satisfy this rule. 
Overall, My finding is that after trying 100+ set of parameters for the Deep Belief Network, the DBN still produces predictions that do not make much sense. Specifically, the distribution of the predictions do not align very closely with the distribution of the y_test variable. This might be because the package has some problem handling this dataset.

